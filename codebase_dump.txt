==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\app.py
==================================================

import streamlit as st
import tempfile
import os
import re  # <--- Added for parsing download tags
from src.core.reasoning_engine import AdaptiveAgent
from src.utils.network import NetworkSentinel

# 1. Page Config
st.set_page_config(
    page_title="Adaptive Agent Pro",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# 2. Corrected CSS for Dark Theme Consistency
st.markdown("""
    <style>
    /* Main Background */
    .stApp {
        background-color: #0E1117;
        color: #FFFFFF;
    }
    
    /* Sidebar Background */
    [data-testid="stSidebar"] {
        background-color: #161B22;
        border-right: 1px solid #30363d;
    }

    /* REMOVE WHITE BACKGROUND FROM CHAT BUBBLES */
    .stChatMessage {
        background-color: #1c2128 !important; /* Dark grey instead of white */
        border: 1px solid #30363d !important;
        border-radius: 10px !important;
        color: #FFFFFF !important;
    }

    /* REMOVE WHITE BACKGROUND FROM STATUS/EXPANDERS */
    .stStatusWidget, .stExpander {
        background-color: #1c2128 !important;
        border: 1px solid #30363d !important;
        color: #FFFFFF !important;
    }
    
    /* Ensure metric text is white */
    [data-testid="stMetricValue"] {
        color: #FFFFFF !important;
    }
    </style>
""", unsafe_allow_html=True)

# Session State Initialization
if "agent" not in st.session_state:
    st.session_state.agent = AdaptiveAgent()
if "messages" not in st.session_state:
    st.session_state.messages = []

# 3. Sidebar: Control Center
with st.sidebar:
    st.header("Control Center")
    st.markdown("---")
    
    st.write("### Network Sentinel")
    sentinel = NetworkSentinel()
    latency = sentinel.ping()
    mode = sentinel.get_mode()
    
    col1, col2 = st.columns(2)
    col1.metric("Latency", f"{int(latency)}ms")
    # size of text
    col2.metric("Mode", mode.split('_')[0][:4].upper() + "...")
    
    st.markdown("---")
    st.write("### Knowledge Base")
    uploaded_file = st.file_uploader("Upload PDF Context", type=["pdf"], label_visibility="collapsed")
    
    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(uploaded_file.getvalue())
            st.session_state.agent.upload_document(tmp.name)
        st.success("Context Loaded")

# 4. Main UI Logic
st.markdown('# Adaptive Reasoning Agent')
st.caption("Mistral-powered agent with network-aware reasoning and native RAG.")

# Display History
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg.get("thought"):
            with st.expander("Internal Thinking"):
                st.markdown(msg["thought"])
        
        st.markdown(msg["content"])
        
        # Check if this message has an attached file to download
        if msg.get("file_path") and os.path.exists(msg["file_path"]):
            with open(msg["file_path"], "rb") as file:
                st.download_button(
                    label="üì• Download Generated Document",
                    data=file,
                    file_name=os.path.basename(msg["file_path"]),
                    mime="application/pdf",
                    key=f"dl_{msg['file_path']}" # Unique key for history buttons
                )

# 5. User Input and Agent Response
if prompt := st.chat_input("Ask a question..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        status_placeholder = st.status("Agent is thinking...", expanded=True)
        
        with status_placeholder:
            placeholder = st.empty()
            full_response = ""
            stream_gen = st.session_state.agent.execute_stream(prompt)
            
            for chunk in stream_gen:
                if chunk.data.choices[0].delta.content:
                    content = chunk.data.choices[0].delta.content
                    full_response += content
                    placeholder.markdown(full_response + "‚ñå")
            
            # --- ADVANCED SANITIZATION LOGIC ---
            thought = ""
            final = full_response

            # 1. Split by the primary competition delimiter
            if "---" in full_response:
                parts = full_response.rsplit("---", 1) 
                thought = parts[0]
                final = parts[1]
            
            # 2. If the model used headers instead of delimiters
            elif "[Polished Answer]" in full_response:
                parts = full_response.split("[Polished Answer]")
                thought = parts[0]
                final = parts[1]

            # 3. CLEANUP: Strip known "System" headers from the UI display
            headers_to_strip = [
                "[Logic Summary]", "[Polished Answer]", "Strategy:", 
                "REASONING_MODE:", "---", "Final Answer:", "Answer:"
            ]
            
            for header in headers_to_strip:
                final = final.replace(header, "")
                thought = thought.replace(header, "")

            # 4. DOWNLOAD HANDLER (New Feature)
            # Regex to find [[DOWNLOAD:path/to/file.pdf]]
            file_path = None
            download_match = re.search(r"\[\[DOWNLOAD:(.*?)\]\]", final)
            
            if download_match:
                file_path = download_match.group(1).strip()
                # Remove the tag from the displayed text so it looks clean
                final = final.replace(download_match.group(0), "")

            status_placeholder.update(label="Reasoning Complete", state="complete", expanded=False)

        # Show the sanitized final answer
        st.markdown(final.strip())
        
        # Show the Download Button immediately if a file was generated
        if file_path and os.path.exists(file_path):
            with open(file_path, "rb") as file:
                st.download_button(
                    label="üì• Download Generated Document",
                    data=file,
                    file_name=os.path.basename(file_path),
                    mime="application/pdf"
                )
        
        # Show the sanitized thought trace
        if thought.strip():
            with st.expander("üîç View Internal Logic"):
                st.info(thought.strip())

        # Save to history (including file_path if it exists)
        st.session_state.messages.append({
            "role": "assistant", 
            "content": final.strip(), 
            "thought": thought.strip(),
            "file_path": file_path  # Save path so button persists on reload
        })

# import streamlit as st
# import tempfile
# import os
# from src.core.reasoning_engine import AdaptiveAgent
# from src.utils.network import NetworkSentinel

# # 1. Page Config
# st.set_page_config(
#     page_title="Adaptive Agent Pro",
#     page_icon="",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )

# # 2. Corrected CSS for Dark Theme Consistency
# st.markdown("""
#     <style>
#     /* Main Background */
#     .stApp {
#         background-color: #0E1117;
#         color: #FFFFFF;
#     }
    
#     /* Sidebar Background */
#     [data-testid="stSidebar"] {
#         background-color: #161B22;
#         border-right: 1px solid #30363d;
#     }

#     /* REMOVE WHITE BACKGROUND FROM CHAT BUBBLES */
#     .stChatMessage {
#         background-color: #1c2128 !important; /* Dark grey instead of white */
#         border: 1px solid #30363d !important;
#         border-radius: 10px !important;
#         color: #FFFFFF !important;
#     }

#     /* REMOVE WHITE BACKGROUND FROM STATUS/EXPANDERS */
#     .stStatusWidget, .stExpander {
#         background-color: #1c2128 !important;
#         border: 1px solid #30363d !important;
#         color: #FFFFFF !important;
#     }
    
#     /* Ensure metric text is white */
#     [data-testid="stMetricValue"] {
#         color: #FFFFFF !important;
#     }
#     </style>
# """, unsafe_allow_html=True)

# # Session State Initialization
# if "agent" not in st.session_state:
#     st.session_state.agent = AdaptiveAgent()
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # 3. Sidebar: Control Center
# with st.sidebar:
#     st.header("Control Center")
#     st.markdown("---")
    
#     st.write("### Network Sentinel")
#     sentinel = NetworkSentinel()
#     latency = sentinel.ping()
#     mode = sentinel.get_mode()
    
#     col1, col2 = st.columns(2)
#     col1.metric("Latency", f"{int(latency)}ms")
#     col2.metric("Mode", mode.split('_')[0][:4].upper() + "...")
    
#     st.markdown("---")
#     st.write("### Knowledge Base")
#     uploaded_file = st.file_uploader("Upload PDF Context", type=["pdf"], label_visibility="collapsed")
    
#     if uploaded_file:
#         with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
#             tmp.write(uploaded_file.getvalue())
#             st.session_state.agent.upload_document(tmp.name)
#         st.success("Context Loaded")

# # 4. Main UI Logic
# st.markdown('# Adaptive Reasoning Agent')
# st.caption("Mistral-powered agent with network-aware reasoning and native RAG.")

# # Display History
# for msg in st.session_state.messages:
#     with st.chat_message(msg["role"]):
#         if msg.get("thought"):
#             with st.expander("Internal Thinking"):
#                 st.markdown(msg["thought"])
#         st.markdown(msg["content"])



# # 5. User Input and Agent Response
# if prompt := st.chat_input("Ask a question..."):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         status_placeholder = st.status("Agent is thinking...", expanded=True)
        
#         with status_placeholder:
#             placeholder = st.empty()
#             full_response = ""
#             stream_gen = st.session_state.agent.execute_stream(prompt)
            
#             for chunk in stream_gen:
#                 if chunk.data.choices[0].delta.content:
#                     content = chunk.data.choices[0].delta.content
#                     full_response += content
#                     placeholder.markdown(full_response + "‚ñå")
            
#             # --- ADVANCED SANITIZATION LOGIC ---
#             thought = ""
#             final = full_response

#             # 1. Split by the primary competition delimiter
#             if "---" in full_response:
#                 # We take the LAST part as the answer, and everything before as logic
#                 parts = full_response.rsplit("---", 1) 
#                 thought = parts[0]
#                 final = parts[1]
            
#             # 2. If the model used headers instead of delimiters
#             elif "[Polished Answer]" in full_response:
#                 parts = full_response.split("[Polished Answer]")
#                 thought = parts[0]
#                 final = parts[1]

#             # 3. CLEANUP: Strip known "System" headers from the UI display
#             headers_to_strip = [
#                 "[Logic Summary]", "[Polished Answer]", "Strategy:", 
#                 "REASONING_MODE:", "---", "Final Answer:", "Answer:"
#             ]
            
#             for header in headers_to_strip:
#                 final = final.replace(header, "")
#                 thought = thought.replace(header, "")

#             status_placeholder.update(label="Reasoning Complete", state="complete", expanded=False)

#         # Show the sanitized final answer
#         st.markdown(final.strip())
        
#         # Show the sanitized thought trace
#         if thought.strip():
#             with st.expander("üîç View Internal Logic"):
#                 st.info(thought.strip())

#         st.session_state.messages.append({
#             "role": "assistant", 
#             "content": final.strip(), 
#             "thought": thought.strip()
#         })

==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\config.py
==================================================

import os
from dotenv import load_dotenv

load_dotenv()

# Strategy Thresholds (in milliseconds)
LATENCY_THRESHOLD_FAST = 300  # Below this = Deep Reasoning allowed
LATENCY_THRESHOLD_POOR = 1000 # Above this = Panic Mode (Fastest possible)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = "AIzaSyBGtEpc4YyD2dLob2_fMEZ0l1VdCGzexy8"
MISTRAL_API_KEY = "hdl7dvGQzeEhwH4wgf8Vkj8ASAQzMS7I"

==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\requirements.txt
==================================================

requests
numpy
openai
python-dotenv
termcolor  # For pretty terminal output
google-genai
mistralai
pypdf 
scikit-learn
fpdf 
openpyxl
duckduckgo-search
ddgs
python-docx

==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\src\core\reasoning_engine.py
==================================================

from mistralai import Mistral
from src.utils.network import NetworkSentinel
from src.tools.native_rag import NativeRAG
from src.tools.document_tool import DocumentTool
from src.tools.web_tool import WebSearchTool 
from config import MISTRAL_API_KEY
import datetime

class AdaptiveAgent:
    def __init__(self):
        self.client = Mistral(api_key=MISTRAL_API_KEY)
        self.sentinel = NetworkSentinel()
        self.rag = NativeRAG()
        self.docs = DocumentTool()
        self.web = WebSearchTool() 
        self.has_context = False

    def upload_document(self, file_path):
        """Standard RAG ingestion logic."""
        self.rag.ingest_pdf(file_path)
        self.has_context = True
    
    def execute_stream(self, user_query):
        """
        THE COMPETITION CORE:
        1. Senses Network (Sentinel).
        2. Detects Intent (Cognitive Routing).
        3. Throttles Tools (Social Intelligence).
        4. Adjusts Reasoning Framework (System 1 vs System 2).
        """
        now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        mode = self.sentinel.get_mode()
        
        # --- 1. Cognitive Intent Routing ---
        greetings = ["hello", "hi", "hey", "assalam", "yo", "greeting"]
        is_social = any(g in user_query.lower().split() for g in greetings)

        print(f"[AGENT] Social: {is_social} | Mode: {mode}")

        intent_response = "NONE"
        if not is_social:
            router_prompt = f"""
            You are an intent classification system.
            Classify the QUERY into exactly ONE category:

            WEB  -> Real-time info (news, weather, stocks).
            RAG  -> Private documents/knowledge base.
            DOC  -> Request to create/generate/download a file (PDF, Excel, Word).
            NONE -> General knowledge.

            STRICT RULES: Output ONLY one word: WEB, RAG, DOC, or NONE.

            QUERY: {user_query}
            """

            intent_response = self.client.chat.complete(
                model="mistral-small-latest",
                messages=[{"role": "user", "content": router_prompt}]
            ).choices[0].message.content.upper()

        
        print(f"[ROUTER] Intent Detected: {intent_response}")

        context_block = ""
        
        # --- 2. Dynamic Strategy Selection ---
        effective_mode = "STANDARD" if is_social else mode
        model = "mistral-large-latest" if effective_mode == "DEEP_REASONING" else "mistral-small-latest"
        print(f"[AGENT] Effective Mode: {effective_mode} | Model: {model}")

        # --- 3. Strategic Tool Execution ---
        if not is_social:
            if "WEB" in intent_response or any(w in user_query.lower() for w in ["now", "today", "weather"]):
                # NEW: Pass the effective mode to control depth
                web_data = self.web.search(user_query, mode=effective_mode)
                context_block += f"\n[LATEST WEB DATA]:\n{web_data}\n"
            
            if "RAG" in intent_response or self.has_context:
                retrieved_text = self.rag.retrieve(user_query)
                context_block += f"\n[DOCUMENT CONTEXT]:\n{retrieved_text}\n"
            
            if "DOC" in intent_response:
                # Use the new generalized handler
                return self._handle_doc_tool(user_query)

        # Framing the 'How' vs 'What'
        prompt = self._get_adaptive_prompt(effective_mode, user_query, context_block, now)
        
        # --- 4. Tool-Specific Fallbacks ---
        if "save" in user_query.lower() and ("pdf" in user_query.lower() or "excel" in user_query.lower() or "word" in user_query.lower()):
            return self._handle_doc_tool(user_query)

        # --- 5. Return Generator ---
        return self.client.chat.stream(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )

    def _get_adaptive_prompt(self, mode, query, context, time):
        """
        MASTER-LEVEL ADAPTIVE REASONING CONTROLLER
        """
        grounding = f"""
    SYSTEM_METADATA:
    - SYSTEM_TIME: {time}

    GROUNDING_RULES:
    1. AVAILABLE_CONTEXT is the highest authority.
    2. If AVAILABLE_CONTEXT exists, prioritize it over prior knowledge.
    3. If context conflicts with prior knowledge, trust context.
    4. If insufficient data, explicitly state uncertainty.
    5. Do NOT fabricate missing details.

    INPUTS:
    - AVAILABLE_CONTEXT: {context if context else "None"}
    - USER_QUERY: {query}
    """

        if mode == "DEEP_REASONING":
            framework = """
    REASONING_MODE: ANALYTICAL (High Deliberation)

    INTERNAL_PROCESS:
    1. Decompose the problem.
    2. Extract verifiable facts from context.
    3. Identify assumptions (if any).
    4. Cross-check for logical consistency.
    5. Synthesize a grounded conclusion.

    OUTPUT_REQUIREMENTS:
    - Provide a structured, logically organized answer.
    - Do NOT expose hidden reasoning steps.
    - Clearly separate explanation and conclusion.
    """
        elif mode == "STANDARD":
            framework = """
    REASONING_MODE: STRUCTURED (Balanced)

    INTERNAL_PROCESS:
    - Identify key facts.
    - Connect them directly to the query.
    - Avoid unnecessary elaboration.

    OUTPUT_REQUIREMENTS:
    - Clear and concise explanation.
    - Direct final answer.
    """
        else:  # FAST_RESPONSE
            framework = """
    REASONING_MODE: HEURISTIC (Low Latency)

    INTERNAL_PROCESS:
    - Use strongest available signal.
    - Prefer brevity over depth.
    - Skip secondary analysis.

    OUTPUT_REQUIREMENTS:
    - Concise answer.
    - No extended explanation unless critical.
    """
        return f"{grounding}\n{framework}"

    def _handle_doc_tool(self, query):
        """
        Smartly generates PDF, WORD, or EXCEL based on user query keywords.
        """
        query_lower = query.lower()
        
        # 1. Determine File Type and Prompt
        if "excel" in query_lower or "spreadsheet" in query_lower or "csv" in query_lower:
            file_type = "EXCEL"
            prompt_instruction = (
                "You are a Data Generator. Output ONLY data in CSV format (comma-separated).\n"
                "Do not use markdown blocks. Just the raw data.\n"
                "Example:\nName, Age, Role\nAlice, 25, Engineer\n"
                "Generate data for: "
            )
        elif "word" in query_lower or "docx" in query_lower:
            file_type = "WORD"
            prompt_instruction = (
                "You are a Document Generator. Output content for a Word Document.\n"
                "Use '# ' for Titles, '## ' for Headings, '* ' for bullets.\n"
                "Write about: "
            )
        else:
            file_type = "PDF" # Default
            prompt_instruction = (
                "You are a PDF Generator. Output content in strict format:\n"
                "- Use '# ' for Main Titles.\n"
                "- Use '## ' for Section Headings.\n"
                "- Use '* ' for bullet points.\n"
                "- Do NOT use bolding symbols like ** inside the text.\n"
                "Write a comprehensive summary about: "
            )

        print(f"[TOOL] üìÑ Generating {file_type} content for: {query}")
        
        # 2. Generate Content
        content = self.client.chat.complete(
            model="mistral-large-latest",
            messages=[{"role": "user", "content": prompt_instruction + query}]
        ).choices[0].message.content
        
        # 3. Create the physical file
        if file_type == "EXCEL":
            path = self.docs.create_excel(content)
        elif file_type == "WORD":
            path = self.docs.create_word(content)
        else:
            path = self.docs.create_pdf(content)
        
        # 4. Yield the response with the HIDDEN TAG [[DOWNLOAD:path]]
        # Create a mock chunk object to satisfy the stream structure
        class ToolChunk:
            def __init__(self, text):
                self.data = type('obj', (object,), {
                    'choices': [type('obj', (object,), {
                        'delta': type('obj', (object,), {'content': text})()
                    })()]
                })()

        yield ToolChunk(f"I have generated your {file_type} document.\n\n[[DOWNLOAD:{path}]]")

# from mistralai import Mistral
# from src.utils.network import NetworkSentinel
# from src.tools.native_rag import NativeRAG
# from src.tools.document_tool import DocumentTool
# from src.tools.web_tool import WebSearchTool 
# from config import MISTRAL_API_KEY
# import datetime

# class AdaptiveAgent:
#     def __init__(self):
#         self.client = Mistral(api_key=MISTRAL_API_KEY)
#         self.sentinel = NetworkSentinel()
#         self.rag = NativeRAG()
#         self.docs = DocumentTool()
#         self.web = WebSearchTool() 
#         self.has_context = False

#     def upload_document(self, file_path):
#         """Standard RAG ingestion logic."""
#         self.rag.ingest_pdf(file_path)
#         self.has_context = True
    
#     def execute_stream(self, user_query):
#         """
#         THE COMPETITION CORE:
#         1. Senses Network (Sentinel).
#         2. Detects Intent (Cognitive Routing).
#         3. Throttles Tools (Social Intelligence).
#         4. Adjusts Reasoning Framework (System 1 vs System 2).
#         """
#         now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
#         mode = self.sentinel.get_mode()
        
#         # --- 1. Cognitive Intent Routing ---
#         # Detecting if the query is a simple greeting to save tokens/latency
#         greetings = ["hello", "hi", "hey", "assalam", "yo", "greeting"]
#         is_social = any(g in user_query.lower().split() for g in greetings)

#         print(f"[AGENT] Social: {is_social} | Mode: {mode}")

#         intent_response = "NONE"
#         if not is_social:
#             router_prompt = f"""
#             You are an intent classification system.
#             Classify the QUERY into exactly ONE category:

#             WEB  -> Real-time info (news, weather, stocks).
#             RAG  -> Private documents/knowledge base.
#             DOC  -> Request to create/generate/download a file, report, or document.
#             NONE -> General knowledge.

#             STRICT RULES: Output ONLY one word: WEB, RAG, DOC, or NONE.

#             QUERY: {user_query}
#             """

#             intent_response = self.client.chat.complete(
#                 model="mistral-small-latest",
#                 messages=[{"role": "user", "content": router_prompt}]
#             ).choices[0].message.content.upper()

        
#         print(f"[ROUTER] Intent Detected: {intent_response}")

#         context_block = ""
        
#         # --- 2. Strategic Tool Execution ---
#         # Logic: Only engage tools if not a greeting and intent matches
#         if not is_social:
#             if "WEB" in intent_response or any(w in user_query.lower() for w in ["now", "today", "weather"]):
#                 web_data = self.web.search(user_query)
#                 context_block += f"\n[LATEST WEB DATA]:\n{web_data}\n"
            
#             if "RAG" in intent_response or self.has_context:
#                 retrieved_text = self.rag.retrieve(user_query)
#                 context_block += f"\n[DOCUMENT CONTEXT]:\n{retrieved_text}\n"
            
#             if "DOC" in intent_response:
#              # We delegate immediately to the PDF handler
#                 return self._handle_pdf_tool(user_query)

#         # --- 3. Dynamic Strategy Selection ---
#         # Force 'STANDARD' for social queries; otherwise use Sentinel's mode
#         effective_mode = "STANDARD" if is_social else mode
#         model = "mistral-large-latest" if effective_mode == "DEEP_REASONING" else "mistral-small-latest"
#         print(f"[AGENT] Effective Mode: {effective_mode} | Model: {model}")
        
#         # Framing the 'How' vs 'What'
#         prompt = self._get_adaptive_prompt(effective_mode, user_query, context_block, now)
        
#         # --- 4. Tool-Specific Fallbacks ---
#         if "save" in user_query.lower() and "pdf" in user_query.lower():
#             return self._handle_pdf_tool(user_query)

#         # --- 5. Return Generator ---
#         return self.client.chat.stream(
#             model=model,
#             messages=[{"role": "user", "content": prompt}]
#         )

#     def _get_adaptive_prompt(self, mode, query, context, time):
#         """
#         MASTER-LEVEL ADAPTIVE REASONING CONTROLLER
#         Controls reasoning depth while enforcing strict grounding hierarchy.
#         """

#         grounding = f"""
#     SYSTEM_METADATA:
#     - SYSTEM_TIME: {time}

#     GROUNDING_RULES:
#     1. AVAILABLE_CONTEXT is the highest authority.
#     2. If AVAILABLE_CONTEXT exists, prioritize it over prior knowledge.
#     3. If context conflicts with prior knowledge, trust context.
#     4. If insufficient data, explicitly state uncertainty.
#     5. Do NOT fabricate missing details.

#     INPUTS:
#     - AVAILABLE_CONTEXT: {context if context else "None"}
#     - USER_QUERY: {query}
#     """

#         if mode == "DEEP_REASONING":
#             framework = """
#     REASONING_MODE: ANALYTICAL (High Deliberation)

#     INTERNAL_PROCESS:
#     1. Decompose the problem.
#     2. Extract verifiable facts from context.
#     3. Identify assumptions (if any).
#     4. Cross-check for logical consistency.
#     5. Synthesize a grounded conclusion.

#     OUTPUT_REQUIREMENTS:
#     - Provide a structured, logically organized answer.
#     - Do NOT expose hidden reasoning steps.
#     - Clearly separate explanation and conclusion.
#     """

#         elif mode == "STANDARD":
#             framework = """
#     REASONING_MODE: STRUCTURED (Balanced)

#     INTERNAL_PROCESS:
#     - Identify key facts.
#     - Connect them directly to the query.
#     - Avoid unnecessary elaboration.

#     OUTPUT_REQUIREMENTS:
#     - Clear and concise explanation.
#     - Direct final answer.
#     """

#         else:  # FAST_RESPONSE
#             framework = """
#     REASONING_MODE: HEURISTIC (Low Latency)

#     INTERNAL_PROCESS:
#     - Use strongest available signal.
#     - Prefer brevity over depth.
#     - Skip secondary analysis.

#     OUTPUT_REQUIREMENTS:
#     - Concise answer.
#     - No extended explanation unless critical.
#     """

#         return f"{grounding}\n{framework}"


#     def _handle_pdf_tool(self, query):
#         """Yields a chunk-compatible generator for the Document Tool."""
#         print(f"[TOOL] üìÑ Generating structured document content for: {query}")
        
#         # 1. Force the LLM to use a strict format we can parse
#         formatting_instructions = (
#             "You are a document generator. Output the content in strict format:\n"
#             "- Use '# ' for Main Titles.\n"
#             "- Use '## ' for Section Headings.\n"
#             "- Use '* ' for bullet points.\n"
#             "- Do NOT use bolding symbols like ** inside the text.\n"
#             "Write a comprehensive summary about: "
#         )
        
#         content = self.client.chat.complete(
#             model="mistral-large-latest",
#             messages=[{"role": "user", "content": formatting_instructions + query}]
#         ).choices[0].message.content
        
#         # 2. Pass this content to the updated PDF creator
#         path = self.docs.create_pdf(content)
        
#         # 3. Create the response object (Standard Streamlit/Mistral chunk format)
#         class ToolChunk:
#             def __init__(self, text):
#                 self.data = type('obj', (object,), {
#                     'choices': [type('obj', (object,), {
#                         'delta': type('obj', (object,), {'content': text})()
#                     })()]
#                 })()

#         # 4. Return the special download tag
#         yield ToolChunk(f"I have generated your document.\n\n[[DOWNLOAD:{path}]]")

==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\src\tools\document_tool.py
==================================================

from fpdf import FPDF
import uuid
import os
import openpyxl
try:
    from docx import Document
except ImportError:
    Document = None  # Handle case where python-docx isn't installed

class DocumentTool:
    def __init__(self):
        if not os.path.exists("data"):
            os.makedirs("data")

    def _sanitize_text(self, text):
        """
        Nuclear option for FPDF compatibility.
        Replaces smart quotes and non-latin characters that crash FPDF.
        """
        replacements = {
            '\u2018': "'",  # Left single quote
            '\u2019': "'",  # Right single quote
            '\u201c': '"',  # Left double quote
            '\u201d': '"',  # Right double quote
            '\u2013': '-',  # En dash
            '\u2014': '--', # Em dash
            '\u2026': '...', # Ellipsis
            '\u2022': '*',   # Bullet
        }
        for char, replacement in replacements.items():
            text = text.replace(char, replacement)
        
        # Final fallback: force into latin-1, replacing unknown chars with '?'
        return text.encode('latin-1', 'replace').decode('latin-1')

    def create_pdf(self, content, filename=None):
        """Generates a PDF with robust encoding handling."""
        if not filename:
            filename = f"doc_{uuid.uuid4().hex[:8]}.pdf"
            
        pdf = FPDF()
        pdf.add_page()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.set_font("Arial", size=11)
        
        # 1. SANITIZE EVERYTHING FIRST
        content = self._sanitize_text(content)
        
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                pdf.ln(3)
                continue

            try:
                # CASE 1: Main Title (#)
                if line.startswith('# '):
                    pdf.set_font("Arial", 'B', 16)
                    pdf.multi_cell(0, 10, line.replace('# ', '').strip())
                    pdf.ln(2)

                # CASE 2: Section Heading (## or ###)
                elif line.startswith('##') or line.startswith('###'):
                    pdf.set_font("Arial", 'B', 12)
                    clean_text = line.replace('### ', '').replace('## ', '').strip()
                    pdf.multi_cell(0, 8, clean_text)
                    pdf.ln(1)
                
                # CASE 3: Bullet Points (* or -)
                elif line.startswith('* ') or line.startswith('- '):
                    pdf.set_font("Arial", '', 11)
                    clean_text = line[2:].strip()
                    current_x = pdf.get_x()
                    pdf.set_x(current_x + 5) 
                    pdf.multi_cell(0, 6, f"{chr(149)} {clean_text}")
                    pdf.set_x(current_x)

                # CASE 4: Standard Text
                else:
                    pdf.set_font("Arial", '', 11)
                    pdf.multi_cell(0, 6, line)
                    
            except Exception as e:
                print(f"[PDF ERROR] Line skipped: {e}")
                # Emergency fallback
                pdf.set_font("Arial", '', 11)
                pdf.multi_cell(0, 6, line)

        path = f"data/{filename}"
        try:
            pdf.output(path)
            return path
        except Exception as e:
            print(f"[CRITICAL PDF FAILURE] {e}")
            return "Error_Generating_PDF.pdf"

    def create_word(self, content, filename=None):
        """Generates a Microsoft Word (.docx) file."""
        if not filename:
            filename = f"doc_{uuid.uuid4().hex[:8]}.docx"
        
        if Document is None:
            return "Error: python-docx not installed"

        doc = Document()
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line: continue
            
            if line.startswith('# '):
                doc.add_heading(line.replace('# ', '').strip(), level=1)
            elif line.startswith('## '):
                doc.add_heading(line.replace('## ', '').replace('### ', '').strip(), level=2)
            elif line.startswith('* ') or line.startswith('- '):
                doc.add_paragraph(line[2:].strip(), style='List Bullet')
            else:
                doc.add_paragraph(line)
                
        path = f"data/{filename}"
        doc.save(path)
        return path

    def create_excel(self, content, filename=None):
        """Generates an Excel (.xlsx) file."""
        if not filename:
            filename = f"sheet_{uuid.uuid4().hex[:8]}.xlsx"
        
        wb = openpyxl.Workbook()
        ws = wb.active
        
        rows = content.strip().split('\n')
        for row in rows:
            columns = [c.strip() for c in row.split(',')]
            ws.append(columns)
            
        path = f"data/{filename}"
        wb.save(path)
        return path


# from fpdf import FPDF
# import uuid
# import os
# import openpyxl
# from docx import Document  # Requires: pip install python-docx

# class DocumentTool:
#     def __init__(self):
#         if not os.path.exists("data"):
#             os.makedirs("data")

#     def create_pdf(self, content, filename=None):
#         """Generates a PDF with formatting."""
#         if not filename:
#             filename = f"doc_{uuid.uuid4().hex[:8]}.pdf"
            
#         pdf = FPDF()
#         pdf.add_page()
#         pdf.set_auto_page_break(auto=True, margin=15)
        
#         # Parse the content line by line
#         lines = content.split('\n')
        
#         for line in lines:
#             line = line.strip()
#             if not line:
#                 pdf.ln(3)  # Add small gap for empty lines
#                 continue

#             try:
#                 # CASE 1: Main Title (#)
#                 if line.startswith('# '):
#                     pdf.set_font("Arial", 'B', 16)
#                     clean_text = line.replace('# ', '').strip()
#                     pdf.multi_cell(0, 10, clean_text)
#                     pdf.ln(2)

#                 # CASE 2: Section Heading (## or ###)
#                 elif line.startswith('##') or line.startswith('###'):
#                     pdf.set_font("Arial", 'B', 12)
#                     clean_text = line.replace('### ', '').replace('## ', '').strip()
#                     pdf.multi_cell(0, 8, clean_text)
#                     pdf.ln(1)
                
#                 # CASE 3: Bullet Points (* or -)
#                 elif line.startswith('* ') or line.startswith('- '):
#                     pdf.set_font("Arial", '', 11)
#                     clean_text = line[2:].strip() # Remove the "* "
#                     current_x = pdf.get_x()
#                     pdf.set_x(current_x + 5) 
#                     pdf.multi_cell(0, 6, f"{chr(149)} {clean_text}")
#                     pdf.set_x(current_x) # Reset indent

#                 # CASE 4: Standard Paragraph Text
#                 else:
#                     pdf.set_font("Arial", '', 11)
#                     pdf.multi_cell(0, 6, line)
                    
#             except Exception as e:
#                 print(f"Error parsing line: {line} | {e}")
#                 pdf.set_font("Arial", '', 11)
#                 pdf.multi_cell(0, 6, line)

#         path = f"data/{filename}"
#         pdf.output(path)
#         return path

#     def create_word(self, content, filename=None):
#         """Generates a Microsoft Word (.docx) file."""
#         if not filename:
#             filename = f"doc_{uuid.uuid4().hex[:8]}.docx"
        
#         doc = Document()
#         lines = content.split('\n')
        
#         for line in lines:
#             line = line.strip()
#             if not line: continue
            
#             # Simple formatting mapping
#             if line.startswith('# '):
#                 doc.add_heading(line.replace('# ', '').strip(), level=1)
#             elif line.startswith('## '):
#                 doc.add_heading(line.replace('## ', '').replace('### ', '').strip(), level=2)
#             elif line.startswith('* ') or line.startswith('- '):
#                 doc.add_paragraph(line[2:].strip(), style='List Bullet')
#             else:
#                 doc.add_paragraph(line)
                
#         path = f"data/{filename}"
#         doc.save(path)
#         return path

#     def create_excel(self, content, filename=None):
#         """Generates an Excel (.xlsx) file from CSV-like text."""
#         if not filename:
#             filename = f"sheet_{uuid.uuid4().hex[:8]}.xlsx"
        
#         wb = openpyxl.Workbook()
#         ws = wb.active
        
#         # Assume LLM sends comma-separated lines
#         rows = content.strip().split('\n')
#         for row in rows:
#             # Clean and split by comma
#             columns = [c.strip() for c in row.split(',')]
#             ws.append(columns)
            
#         path = f"data/{filename}"
#         wb.save(path)
#         return path

# # from fpdf import FPDF
# # import uuid
# # import os

# # class DocumentTool:
# #     def __init__(self):
# #         if not os.path.exists("data"):
# #             os.makedirs("data")

# #     def create_pdf(self, content, filename=None):
# #         if not filename:
# #             filename = f"doc_{uuid.uuid4().hex[:8]}.pdf"
            
# #         pdf = FPDF()
# #         pdf.add_page()
# #         pdf.set_auto_page_break(auto=True, margin=15)
        
# #         # Parse the content line by line
# #         lines = content.split('\n')
        
# #         for line in lines:
# #             line = line.strip()
# #             if not line:
# #                 pdf.ln(3)  # Add small gap for empty lines
# #                 continue

# #             try:
# #                 # CASE 1: Main Title (#)
# #                 if line.startswith('# '):
# #                     pdf.set_font("Arial", 'B', 16)
# #                     # Clean the markdown symbol
# #                     clean_text = line.replace('# ', '').strip()
# #                     pdf.multi_cell(0, 10, clean_text)
# #                     pdf.ln(2)

# #                 # CASE 2: Section Heading (## or ###)
# #                 elif line.startswith('##') or line.startswith('###'):
# #                     pdf.set_font("Arial", 'B', 12)
# #                     clean_text = line.replace('### ', '').replace('## ', '').strip()
# #                     pdf.multi_cell(0, 8, clean_text)
# #                     pdf.ln(1)
                
# #                 # CASE 3: Bullet Points (* or -)
# #                 elif line.startswith('* ') or line.startswith('- '):
# #                     pdf.set_font("Arial", '', 11)
# #                     clean_text = line[2:].strip() # Remove the "* "
# #                     # Simulate a bullet point with indent
# #                     current_x = pdf.get_x()
# #                     pdf.set_x(current_x + 5) 
# #                     pdf.multi_cell(0, 6, f"{chr(149)} {clean_text}")
# #                     pdf.set_x(current_x) # Reset indent

# #                 # CASE 4: Standard Paragraph Text
# #                 else:
# #                     pdf.set_font("Arial", '', 11)
# #                     pdf.multi_cell(0, 6, line)
                    
# #             except Exception as e:
# #                 print(f"Error parsing line: {line} | {e}")
# #                 # Fallback ensures text isn't lost
# #                 pdf.set_font("Arial", '', 11)
# #                 pdf.multi_cell(0, 6, line)

# #         path = f"data/{filename}"
# #         pdf.output(path)
# #         return path

==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\src\tools\native_rag.py
==================================================

import numpy as np
from pypdf import PdfReader
from mistralai import Mistral
from sklearn.metrics.pairwise import cosine_similarity
from config import MISTRAL_API_KEY

class NativeRAG:
    def __init__(self):
        self.client = Mistral(api_key=MISTRAL_API_KEY)
        self.vector_db = [] # In-memory storage: [{"text": str, "vector": array}]
        self.chunk_size = 500 # Characters per chunk

    def ingest_pdf(self, file_path):
        """
        1. Parse PDF
        2. Chunk Text
        3. Vectorize (Embed)
        """
        print(f"[RAG] üìÇ Ingesting {file_path}...")
        
        # 1. Read PDF
        reader = PdfReader(file_path)
        full_text = ""
        for page in reader.pages:
            full_text += page.extract_text() + "\n"
            
        # 2. Manual Chunking (Sliding Window)
        chunks = [full_text[i:i+self.chunk_size] for i in range(0, len(full_text), self.chunk_size)]
        
        # 3. Vectorize in Batch
        # Mistral embeddings API
        embeddings_batch = self.client.embeddings.create(
            model="mistral-embed",
            inputs=chunks
        )
        
        # Store in "DB"
        self.vector_db = [] # Reset for demo (or append for multi-doc)
        for i, chunk in enumerate(chunks):
            self.vector_db.append({
                "text": chunk,
                "vector": embeddings_batch.data[i].embedding
            })
        
        print(f"[RAG] ‚úÖ Indexed {len(chunks)} chunks.")

    def retrieve(self, query, top_k=3):
        """
        1. Embed Query
        2. Math (Cosine Similarity)
        3. Return Top K
        """
        if not self.vector_db:
            return ""

        # Embed User Query
        query_emb = self.client.embeddings.create(
            model="mistral-embed",
            inputs=[query]
        ).data[0].embedding

        # Convert list to numpy array for speed
        db_vectors = np.array([item["vector"] for item in self.vector_db])
        query_vector = np.array([query_emb])

        # Calculate Similarity (Dot Product / Norm)
        similarities = cosine_similarity(query_vector, db_vectors)[0]
        
        # Get Top K indices
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        # Construct Context String
        context = "\n---\n".join([self.vector_db[i]["text"] for i in top_indices])
        return context

==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\src\tools\web_tool.py
==================================================

from ddgs import DDGS
import datetime

class WebSearchTool:
    def __init__(self):
        # We initialize a fresh instance per search to avoid session staleness
        pass

    def search(self, query, mode="STANDARD"):
        """
        Adaptive Search:
        - FAST_RESPONSE: Shallow search (3 results), Text only.
        - STANDARD/DEEP: Deep search (8 results), Text + News fallback.
        """
        # 1. Clean the query
        clean_query = query.replace("now", "").split("2026")[0].strip()
        
        # 2. Adjust Depth based on Network Mode
        if mode == "FAST_RESPONSE":
            max_results = 3
        else:
            max_results = 8
            
        print(f"[TOOL] üåê Adaptive Search ({mode}): {clean_query} | Limit: {max_results}")
        
        try:
            with DDGS() as ddgs:
                # 3. Try 'Text' first
                results = list(ddgs.text(clean_query, region="wt-wt", max_results=max_results))
                
                # 4. If Text fails, immediately try 'News'
                if not results:
                    print("[TOOL] ‚ö†Ô∏è Text search empty. Trying Live News...")
                    results = list(ddgs.news(clean_query, region="wt-wt", max_results=max_results))

                if not results:
                    return "No live data found. Search engines returned empty results."

                # 5. Format for the LLM
                formatted_results = []
                for r in results:
                    # 'body' in text search, 'description' in news
                    content = r.get('body') or r.get('description', 'No details available.')
                    formatted_results.append(f"Title: {r['title']}\nSource: {r['href']}\nSnippet: {content}")

                return "\n\n".join(formatted_results)

        except Exception as e:
            print(f"[ERROR] Web Tool Critical Failure: {e}")
            return f"Search Error: The search provider is unreachable. (Detail: {str(e)[:50]})"

# from ddgs import DDGS  # Use the new import
# import datetime

# class WebSearchTool:
#     def __init__(self):
#         # We initialize a fresh instance per search to avoid session staleness
#         pass

#     def search(self, query, max_results=5):
#         # 1. Clean the query
#         # Remove timestamps that might confuse the search engine's indexing
#         clean_query = query.replace("now", "").split("2026")[0].strip()
        
#         print(f"[TOOL] üåê Multi-Stage Search: {clean_query}")
        
#         try:
#             with DDGS() as ddgs:
#                 # 2. Try 'Text' first
#                 results = list(ddgs.text(clean_query, region="wt-wt", max_results=max_results))
                
#                 # 3. If Text fails, immediately try 'News' (Higher reliability for real-time)
#                 if not results:
#                     print("[TOOL] ‚ö†Ô∏è Text search empty. Trying Live News...")
#                     results = list(ddgs.news(clean_query, region="wt-wt", max_results=max_results))

#                 if not results:
#                     return "No live data found. Search engines returned empty results."

#                 # 4. Format for the LLM
#                 formatted_results = []
#                 for r in results:
#                     # 'body' in text search, 'description' in news
#                     content = r.get('body') or r.get('description', 'No details available.')
#                     formatted_results.append(f"Title: {r['title']}\nSource: {r['href']}\nSnippet: {content}")

#                 return "\n\n".join(formatted_results)

#         except Exception as e:
#             print(f"[ERROR] Web Tool Critical Failure: {e}")
#             return f"Search Error: The search provider is unreachable. (Detail: {str(e)[:50]})"

==================================================
FILE PATH: C:\Users\Nabeel\Desktop\Devcon\adaptive-reasoning-agent\src\utils\network.py
==================================================

import time
import requests
import statistics
from termcolor import colored

class NetworkSentinel:
    def __init__(self, target_url="https://1.1.1.1"):
        self.target = target_url

    def ping(self, runs=3):
        """
        Pings a high-availability server to estimate RTT (Round Trip Time).
        Returns: Average latency in milliseconds (ms).
        """
        latencies = []
        try:
            for _ in range(runs):
                start = time.time()
                requests.get(self.target, timeout=2)
                latencies.append((time.time() - start) * 1000)
            
            avg_latency = statistics.mean(latencies)
            return avg_latency
        except requests.RequestException:
            return 9999.0  # Max latency on failure

    def get_mode(self):
        """
        Decides the reasoning strategy based on current network health.
        """
        latency = self.ping()
        
        if latency < 300:
            status = colored(f"STRONG ({int(latency)}ms)", "green")
            mode = "DEEP_REASONING"
        elif latency < 1000:
            status = colored(f"MODERATE ({int(latency)}ms)", "yellow")
            mode = "STANDARD"
        else:
            status = colored(f"POOR ({int(latency)}ms)", "red")
            mode = "FAST_RESPONSE"
            
        print(f"[SYSTEM] Network Status: {status} -> Mode: {mode}")
        return mode

